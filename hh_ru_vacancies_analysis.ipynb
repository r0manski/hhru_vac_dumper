{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing vacancies description from HH.RU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import http.client\n",
    "import datetime\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from operator import itemgetter\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "from nltk.corpus import stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supporting Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to run the folowing commands in your terminal/command-line to download the required ntlk packegaes and stopwords corpus on your machine:\n",
    "    >>> import nltk\n",
    "    >>> nltk.download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Providing headers as recomennded in API Documentation\n",
    "headers = {\"User-Agent\": \"HH-User-Agent\"}\n",
    "\n",
    "#function to get all the related vacancy ids\n",
    "def get_vacancy_ids(keyword):\n",
    "    vacancy_ids = []\n",
    "    conn = http.client.HTTPSConnection(\"api.hh.ru\")\n",
    "    per_page = 100 #100 is a maximum allowed by API\n",
    "    page = 0\n",
    "    count = per_page\n",
    "    date_from = (datetime.datetime.now() - datetime.timedelta(days=29)).strftime('%Y-%m-%dT%H:%M:%S')\n",
    "    date_to = datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
    "    area_id = 113 #Russia\n",
    "    \n",
    "    while count == per_page:\n",
    "        path = (\"/vacancies?text={}&area={}&per_page={}&date_from={}&date_to={}&page={}\"\n",
    "                .format(keyword,area_id,per_page, date_from, date_to, page))\n",
    "        \n",
    "        conn.request(\"GET\", path, headers=headers)\n",
    "        resp = conn.getresponse()\n",
    "        if resp.status != 200:\n",
    "        # something went wrong\n",
    "            raise ValueError('API error happened.')\n",
    "        vacancies = resp.read()\n",
    "        conn.close()\n",
    "\n",
    "        count = len(json.loads(vacancies)['items'])\n",
    "        page = page+1\n",
    "        for item in json.loads(vacancies)['items']:\n",
    "            vacancy_ids.append(item['id'])\n",
    "    return vacancy_ids\n",
    "\n",
    "\n",
    "#function to retrieve vacancy description by vacancy id and save it to a txt file.\n",
    "def get_vacancies(vacancy_ids, ):\n",
    "    for vac_id in vacancy_ids:\n",
    "        conn = http.client.HTTPSConnection(\"api.hh.ru\")\n",
    "        conn.request(\"GET\", \"/vacancies/{}\".format(vac_id), headers=headers)\n",
    "        resp = conn.getresponse()\n",
    "        if resp.status != 200:\n",
    "        # something went wrong\n",
    "            raise ValueError('API error happened.')\n",
    "        vacancy_txt = resp.read()\n",
    "        conn.close()\n",
    "        vacancy = json.loads(vacancy_txt)\n",
    "        #cleaning description out of html tags and other irrelevant charachters\n",
    "        clean_desc = ''\n",
    "        desc = re.sub('<[^>]*>', '', vacancy['description'].lower())\n",
    "        desc = re.sub('ur[^a-zа-я]+', ' ', desc, re.UNICODE)        \n",
    "        words = desc.split()\n",
    "        for word in words:\n",
    "                    if len(word.strip()) > 2:\n",
    "                        clean_desc = desc + \" \" + word\n",
    "        \n",
    "        with open('corpus.txt', 'a') as f:\n",
    "            f.write(\" \" + clean_desc)\n",
    "            f.close\n",
    "    print('file corpus.txt with vacancies descriptions is created in the working directory')\n",
    "\n",
    "#TODO: does not work as expected yet.    \n",
    "def tokenize_me(text):\n",
    "    \n",
    "    #nltk tokenization\n",
    "    tokens = nltk.word_tokenize(text_string)\n",
    "\n",
    "    #cleaning words\n",
    "    tokens = [i.replace(\"«\", \"\").replace(\"»\", \"\") for i in tokens]\n",
    "    tokens = re.findall(r'\\b[a-zа-я]{3,15}\\b', text_string)\n",
    "\n",
    "    #deleting stop_words\n",
    "    stopWords = set()\n",
    "    stopWords.update(stopwords.words('english'),stopwords.words('russian'))\n",
    "    tokens = [i for i in tokens if ( i not in stopWords)]\n",
    "\n",
    "    return tokens        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting vacancies descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#getting vacancy ids for Data Scientist keyword search\n",
    "ids = get_vacancy_ids(\"Data+Scientist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file corpus.txt with vacancies descriptions is created in the working directory\n"
     ]
    }
   ],
   "source": [
    "#getting vacancies descr file... Warning!!! takes time...\n",
    "get_vacancies(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reading vac desc file\n",
    "document_text = open('corpus.txt', 'r')\n",
    "text_string = document_text.read()\n",
    "document_text.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenizing the text with vac desc\n",
    "tokens = tokenize_me(text_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# creating dict with token - frequency items\n",
    "frequency_dict = {}\n",
    "\n",
    "for word in tokens:\n",
    "    count = frequency_dict.get(word,0)\n",
    "    frequency_dict[word] = count + 1\n",
    "     \n",
    "frequency_list = frequency_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dictionary sorted by value in descending order\n",
    "sorted_frequency_dict = OrderedDict(sorted(frequency_dict.items(), key=lambda t: t[1],reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "данных 567\n",
      "data 545\n",
      "опыт 542\n",
      "работы 518\n",
      "знание 278\n",
      "python 255\n",
      "обучения 228\n",
      "моделей 212\n",
      "машинного 191\n",
      "анализа 176\n",
      "компании 164\n",
      "задач 155\n",
      "learning 152\n",
      "области 147\n",
      "задачи 145\n",
      "алгоритмов 136\n",
      "sql 133\n",
      "бизнес 128\n",
      "experience 127\n",
      "анализ 122\n",
      "machine 121\n",
      "science 117\n",
      "требования 116\n",
      "spark 116\n",
      "решения 115\n",
      "условия 114\n",
      "разработка 113\n",
      "участие 112\n",
      "умение 103\n",
      "построение 103\n",
      "возможность 102\n",
      "работа 102\n",
      "big 102\n",
      "team 96\n",
      "образование 88\n",
      "business 86\n",
      "hadoop 85\n",
      "разработки 84\n",
      "обязанности 83\n",
      "методов 83\n",
      "quot 83\n",
      "work 82\n",
      "дмс 81\n",
      "обучение 81\n",
      "понимание 80\n",
      "scientist 80\n",
      "обработки 77\n",
      "модели 77\n",
      "офис 76\n"
     ]
    }
   ],
   "source": [
    "# slicing 50 most frequent words\n",
    "first_50 = itertools.islice(sorted_frequency_dict.items(), 0, 49)\n",
    "for key, value in first_50:\n",
    "    print (key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO next: Add a phrases frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to run the folowing commands in your terminal/command-line to download the stopwords corpus on your machine:\n",
    "\n",
    "    >>> import nltk\n",
    "    >>> nltk.download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "document_text = open('corpus.txt', 'r')\n",
    "text_string = document_text.read()\n",
    "document_text.close()\n",
    "\n",
    "#nltk tokenization\n",
    "tokens = nltk.word_tokenize(text_string)\n",
    "\n",
    "#cleaning words\n",
    "tokens = [i.replace(\"«\", \"\").replace(\"»\", \"\") for i in tokens]\n",
    "tokens = re.findall(r'\\b[a-zа-я]{3,15}\\b', text_string)\n",
    "\n",
    "#deleting stop_words\n",
    "stopWords = set()\n",
    "stopWords.update(stopwords.words('english'),stopwords.words('russian'))\n",
    "tokens = [i for i in tokens if ( i not in stopWords)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
