{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing vacancies description from HH.RU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import http.client\n",
    "import datetime\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from operator import itemgetter\n",
    "from collections import OrderedDict\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supporting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Providing headers as recomennded in API Documentation\n",
    "headers = {\"User-Agent\": \"HH-User-Agent\"}\n",
    "\n",
    "#function to get all the related vacancy ids\n",
    "def get_vacancy_ids(keyword):\n",
    "    vacancy_ids = []\n",
    "    conn = http.client.HTTPSConnection(\"api.hh.ru\")\n",
    "    per_page = 100 #100 is a maximum allowed by API\n",
    "    page = 0\n",
    "    count = per_page\n",
    "    date_from = (datetime.datetime.now() - datetime.timedelta(days=29)).strftime('%Y-%m-%dT%H:%M:%S')\n",
    "    date_to = datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
    "    area_id = 113 #Russia\n",
    "    \n",
    "    while count == per_page:\n",
    "        path = (\"/vacancies?text={}&area={}&per_page={}&date_from={}&date_to={}&page={}\"\n",
    "                .format(keyword,area_id,per_page, date_from, date_to, page))\n",
    "        \n",
    "        conn.request(\"GET\", path, headers=headers)\n",
    "        resp = conn.getresponse()\n",
    "        if resp.status != 200:\n",
    "        # something went wrong\n",
    "            raise ValueError('API error happened.')\n",
    "        vacancies = resp.read()\n",
    "        conn.close()\n",
    "\n",
    "        count = len(json.loads(vacancies)['items'])\n",
    "        page = page+1\n",
    "        for item in json.loads(vacancies)['items']:\n",
    "            vacancy_ids.append(item['id'])\n",
    "    return vacancy_ids\n",
    "\n",
    "\n",
    "#function to retrieve vacancy description by vacancy id and save it to a txt file.\n",
    "def get_vacancies(vacancy_ids, ):\n",
    "    for vac_id in vacancy_ids:\n",
    "        conn = http.client.HTTPSConnection(\"api.hh.ru\")\n",
    "        conn.request(\"GET\", \"/vacancies/{}\".format(vac_id), headers=headers)\n",
    "        resp = conn.getresponse()\n",
    "        if resp.status != 200:\n",
    "        # something went wrong\n",
    "            raise ValueError('API error happened.')\n",
    "        vacancy_txt = resp.read()\n",
    "        conn.close()\n",
    "        vacancy = json.loads(vacancy_txt)\n",
    "        #cleaning description out of html tags and other irrelevant charachters\n",
    "        clean_desc = ''\n",
    "        desc = re.sub('<[^>]*>', '', vacancy['description'].lower())\n",
    "        desc = re.sub('ur[^a-zа-я]+', ' ', desc, re.UNICODE)        \n",
    "        words = desc.split()\n",
    "        for word in words:\n",
    "                    if len(word.strip()) > 2:\n",
    "                        clean_desc = desc + \" \" + word\n",
    "        \n",
    "        with open('corpus.txt', 'a') as f:\n",
    "            f.write(\" \" + clean_desc)\n",
    "            f.close\n",
    "    print('file corpus.txt with vacancies descriptions is created in the working directory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting vacancies descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#getting vacancy ids for Data Scientist keyword search\n",
    "ids = get_vacancy_ids(\"Data+Scientist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#getting vacancies descr file... Warning!!! takes time...\n",
    "get_vacancies(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "document_text = open('corpus.txt', 'r')\n",
    "text_string = document_text.read()\n",
    "wordlist = re.findall(r'\\b[a-zа-я]{3,15}\\b', text_string)\n",
    "\n",
    "frequency_dict = {}\n",
    "\n",
    "for word in wordlist:\n",
    "    count = frequency_dict.get(word,0)\n",
    "    frequency_dict[word] = count + 1\n",
    "     \n",
    "frequency_list = frequency_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dictionary sorted by value in descending order\n",
    "sorted_frequency_dict = OrderedDict(sorted(frequency_dict.items(), key=lambda t: t[1],reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# slicing 50 most frequent words\n",
    "first_50 = itertools.islice(sorted_frequency_dict.items(), 0, 49)\n",
    "for key, value in first_50:\n",
    "    print (key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO next: Remove English and Russian stop words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
