{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing vacancies description from HH.RU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import http.client\n",
    "import datetime\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from operator import itemgetter\n",
    "from collections import OrderedDict\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supporting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Providing headers as recomennded in API Documentation\n",
    "headers = {\"User-Agent\": \"HH-User-Agent\"}\n",
    "\n",
    "#function to get all the related vacancy ids\n",
    "def get_vacancy_ids(keyword):\n",
    "    vacancy_ids = []\n",
    "    conn = http.client.HTTPSConnection(\"api.hh.ru\")\n",
    "    per_page = 100 #100 is a maximum allowed by API\n",
    "    page = 0\n",
    "    count = per_page\n",
    "    date_from = (datetime.datetime.now() - datetime.timedelta(days=29)).strftime('%Y-%m-%dT%H:%M:%S')\n",
    "    date_to = datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
    "    area_id = 113 #Russia\n",
    "    \n",
    "    while count == per_page:\n",
    "        path = (\"/vacancies?text={}&area={}&per_page={}&date_from={}&date_to={}&page={}\"\n",
    "                .format(keyword,area_id,per_page, date_from, date_to, page))\n",
    "        \n",
    "        conn.request(\"GET\", path, headers=headers)\n",
    "        resp = conn.getresponse()\n",
    "        if resp.status != 200:\n",
    "        # something went wrong\n",
    "            raise ValueError('API error happened.')\n",
    "        vacancies = resp.read()\n",
    "        conn.close()\n",
    "\n",
    "        count = len(json.loads(vacancies)['items'])\n",
    "        page = page+1\n",
    "        for item in json.loads(vacancies)['items']:\n",
    "            vacancy_ids.append(item['id'])\n",
    "    return vacancy_ids\n",
    "\n",
    "\n",
    "#function to retrieve vacancy description by vacancy id and save it to a txt file.\n",
    "def get_vacancies(vacancy_ids, ):\n",
    "    for vac_id in vacancy_ids:\n",
    "        conn = http.client.HTTPSConnection(\"api.hh.ru\")\n",
    "        conn.request(\"GET\", \"/vacancies/{}\".format(vac_id), headers=headers)\n",
    "        resp = conn.getresponse()\n",
    "        if resp.status != 200:\n",
    "        # something went wrong\n",
    "            raise ValueError('API error happened.')\n",
    "        vacancy_txt = resp.read()\n",
    "        conn.close()\n",
    "        vacancy = json.loads(vacancy_txt)\n",
    "        #cleaning description out of html tags and other irrelevant charachters\n",
    "        clean_desc = ''\n",
    "        desc = re.sub('<[^>]*>', '', vacancy['description'].lower())\n",
    "        desc = re.sub('ur[^a-zа-я]+', ' ', desc, re.UNICODE)        \n",
    "        words = desc.split()\n",
    "        for word in words:\n",
    "                    if len(word.strip()) > 2:\n",
    "                        clean_desc = desc + \" \" + word\n",
    "        \n",
    "        with open('corpus.txt', 'a') as f:\n",
    "            f.write(\" \" + clean_desc)\n",
    "            f.close\n",
    "    print('file corpus.txt with vacancies descriptions is created in the working directory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting vacancies descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#getting vacancy ids for Data Scientist keyword search\n",
    "ids = get_vacancy_ids(\"Data+Scientist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file corpus.txt with vacancies descriptions is created in the working directory\n"
     ]
    }
   ],
   "source": [
    "#getting vacancies descr file... Warning!!! takes time...\n",
    "get_vacancies(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "document_text = open('corpus.txt', 'r')\n",
    "text_string = document_text.read()\n",
    "wordlist = re.findall(r'\\b[a-zа-я]{3,15}\\b', text_string)\n",
    "\n",
    "frequency_dict = {}\n",
    "\n",
    "for word in wordlist:\n",
    "    count = frequency_dict.get(word,0)\n",
    "    frequency_dict[word] = count + 1\n",
    "     \n",
    "frequency_list = frequency_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dictionary sorted by value in descending order\n",
    "sorted_frequency_dict = OrderedDict(sorted(frequency_dict.items(), key=lambda t: t[1],reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and 704\n",
      "данных 567\n",
      "для 561\n",
      "data 545\n",
      "опыт 542\n",
      "работы 518\n",
      "the 395\n",
      "знание 278\n",
      "python 255\n",
      "обучения 228\n",
      "with 223\n",
      "моделей 212\n",
      "машинного 191\n",
      "анализа 176\n",
      "for 165\n",
      "компании 164\n",
      "или 161\n",
      "задач 155\n",
      "learning 152\n",
      "области 147\n",
      "задачи 145\n",
      "алгоритмов 136\n",
      "sql 133\n",
      "бизнес 128\n",
      "experience 127\n",
      "что 125\n",
      "анализ 122\n",
      "machine 121\n",
      "science 117\n",
      "требования 116\n",
      "spark 116\n",
      "решения 115\n",
      "условия 114\n",
      "разработка 113\n",
      "участие 112\n",
      "умение 103\n",
      "построение 103\n",
      "возможность 102\n",
      "работа 102\n",
      "big 102\n",
      "как 101\n",
      "you 97\n",
      "team 96\n",
      "будет 94\n",
      "нас 93\n",
      "образование 88\n",
      "есть 88\n",
      "are 87\n",
      "business 86\n"
     ]
    }
   ],
   "source": [
    "# slicing 50 most frequent words\n",
    "first_50 = itertools.islice(sorted_frequency_dict.items(), 0, 49)\n",
    "for key, value in first_50:\n",
    "    print (key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO next: Remove English and Russian stop words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
